#+TITLE: Elisp Ollama SDK

* Introduction

This project aims to be featureful elisp sdk for [[https://ollama.ai/][Ollama]]. There are existing projects for working with Ollama such as [[https://github.com/zweifisch/ollama][zweifish/ollama]] and [[https://github.com/s-kostyaev/ellama][ellama]]. I want the Elisp Ollama SDK to be a more minimal abstraction over ollama that allows elisp developers to integrate open source (or at least open weight) LLMs into elisp projects using elisp. It will offer much the same level of functionality that the [[https://github.com/ollama/ollama-python][ollama-python]] library offers, but in emacs lisp.

* Usage Examples
First, clone the repo and load the ~ollama.el~ file.
#+begin_src emacs-lisp :session ollamatest
(load-file "./ollama.el")
#+end_src

#+RESULTS:
: t

** Synchronous
This approach makes synchronous (blocking) calls to the Ollama API. This is not recommended if you are using a slower model or generating a lot of tokens as it will freeze your emacs session until it is complete.
*** Completions


#+begin_src emacs-lisp
(pp
 (ollama-generate-completion
  "mistral:latest"
  "why is the sky blue?"
  :options '(:num_predict 10)))
#+end_src

: ((model . "mistral:latest")
:  (created_at . "2024-02-24T14:06:24.157224Z")
:  (response . " The color of the sky appears blue due to a")
:  (done . t)
:  (context .
:           [733 16289 28793 28705 2079 349 272 7212 5045 28804 733 28748 16289 28793 415 3181 302 272 7212 8045 5045 2940 298 264])
:  (total_duration . 7597444792)
:  (load_duration . 7109315792)
:  (prompt_eval_count . 15)
:  (prompt_eval_duration . 176668000)
:  (eval_count . 10)
:  (eval_duration . 310997000))

*** Chat Completions

A chat is represented as a list of alists, where each alist contains two key-value pairs: =role= and =content=. The =role= specifies who is speaking (~system~, ~user~, or ~assistant~), and ~content~ contains the message text. For example:

#+begin_src emacs-lisp
'((("role" . "system") ("content" . "Initial system message."))
  (("role" . "user") ("content" . "Can you help me with elisp programming?"))
  (("role" . "assistant")
   ("content"
    .
    "Absolutely, I'd be happy to help you with Emacs Lisp (Elisp)."))
  (("role" . "user") ("content" . "Great, how do I start?")))
#+end_src

In the context of the ~ollama-generate-chat-completion~ function, this looks like:

#+begin_src emacs-lisp
(pp
 (ollama-generate-chat-completion
  "mistral:latest"
  '((("role" . "system") ("content" . "You are a helpful assistant."))
    (("role" . "user") ("content" . "Can you help me with elisp programming?")))
  :options '((temperature . 0.7) (num_predict . 20))
  :stream
  :json-false))
#+end_src

: ((model . "mistral:latest")
:  (created_at . "2024-02-24T14:07:20.0499Z")
:  (message
:   (role . "assistant")
:   (content . " Absolutely, I'll do my best to assist you with Emacs Lisp (Elisp"))
:  (done . t)
:  (total_duration . 1102761667)
:  (load_duration . 1710167)
:  (prompt_eval_count . 19)
:  (prompt_eval_duration . 370450000)
:  (eval_count . 20)
:  (eval_duration . 729234000))

** Asynchronous
This approach will not block your emacs session. However, it will be revised heavily in the near future. I am still working on finding a good usage pattern. At present, it prints the output to a buffer called ~*ollama-output*~.

Invoking the function:

#+begin_src emacs-lisp
(async-ollama-generate-completion
 "mistral:latest"
 "why is the sky blue?"
 :options '(:num_predict 10))

  #+end_src

  #+RESULTS:
  : #<buffer  *http localhost:11434*-158912>

Getting the output from the buffer:

#+begin_src emacs-lisp
 (let ((content
        (with-current-buffer "*ollama-response*"
          ;; Capture the buffer's content.
          (buffer-string))))
   ;; Clear the buffer after capturing its content.
   (with-current-buffer "*ollama-response*"
     (erase-buffer))
   ;; Ensure json-parse-string returns an alist.
   (pp(json-parse-string content :object-type 'alist)))
#+end_src

: ((model . "mistral:latest")
:  (created_at . "2024-02-26T02:46:02.050339Z")
:  (response . " The color of the sky appears blue due to a")
:  (done . t)
:  (context .
:           [733 16289 28793 28705 2079 349 272 7212 5045 28804 733 28748 16289 28793 415 3181 302 272 7212 8045 5045 2940 298 264])
:  (total_duration . 388790958)
:  (load_duration . 1314250)
:  (prompt_eval_duration . 137733000)
:  (eval_count . 10)
:  (eval_duration . 249134000))


#+begin_src emacs-lisp
(async-ollama-generate-chat-completion "mistral:latest"
             '((("role" . "system") ("content" . "You are a helpful assistant."))
               (("role" . "user") ("content" . "Can you help me with elisp programming?")))
             :options '((temperature . 0.7) (num_predict . 20))
             :stream :json-false)
  #+end_src

  #+RESULTS:
  : #<buffer  *http localhost:11434*-667842>


#+begin_src emacs-lisp
(let ((content
       (with-current-buffer "*ollama-response*"
         ;; Capture the buffer's content.
         (buffer-string))))
  ;; Clear the buffer after capturing its content.
  (with-current-buffer "*ollama-response*"
    (erase-buffer))
  ;; Ensure json-parse-string returns an alist.
  (pp(json-parse-string content :object-type 'alist)))
#+end_src

: ((model . "mistral:latest")
:  (created_at . "2024-02-26T02:47:11.613687Z")
:  (message
:   (role . "assistant")
:   (content . " Absolutely, I'll do my best to help you with Emacs Lisp (Elisp"))
:  (done . t)
:  (total_duration . 806273708)
:  (load_duration . 1542208)
:  (prompt_eval_count . 19)
:  (prompt_eval_duration . 265775000)
:  (eval_count . 20)
:  (eval_duration . 536168000))

* Project Status
** [2024-02-25 Sun]
I revised the async function calls to use the ~url~ library instead of calling on ~curl~. Currently the async requests print the json output to a buffer; a callback can be used to determine what is done with the output. I am still thinking about the best usage pattern here but the ability to define custom callbacks should offer some flexibility.
** [2024-02-24 Sat]
I updated the examples in the readme for soem clarity. Next big step is still to refactor the asynchronouc calls and think of a better usage pattern. I also need to add support for embeddings models.
** [2024-02-10 Sat]
I have basic working versions of synchronous and asynchronous calls to the ~generate~ and ~chat~ endpoints, along with examples in the readme.

Next up:
- try to refactor the asynchronous calls to use `url` rather than starting a `curl` process
- get streaming working
- implement the other endpoints for e.g. listing models
** [2024-02-08 Thu]
I'm experimenting with different ways of handling asynchronous results. For now, in the spirit of maximum flexibility, I'm going to allow the user to define custom callbacks specifying what should be done with the results (in addition to providing a default one, which will print the results to a buffer, and some alternatives to e.g. save to variables or print the results to the minibuffer).
** [2024-02-06 Tue]
I've added a base function for sending requests, a simple process filter for printing the results to a buffer, and specific functions for completions and chat completions that take elisp objects as inputs and pass the correct JSON on to the API. 
** [2024-02-04 Sun]
The project has only just begun! Though I have used ollama via elisp fairly extensively in the past so I expect to make fairly rapid progress.
